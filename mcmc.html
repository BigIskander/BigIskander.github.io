---
layout: default
title: Реализация алгоритма Metropolis-Hastings MCMC в R для случая нескольких переменных.
formula: "Y"
code: "Y"
date: "2022-12-03"
---
<h1>Реализация алгоритма Metropolis-Hastings MCMC в R для случая нескольких переменных.</h1>
<p>{{ page.date | date_to_string }}</p>
<p>Это реализация алгоритма Metropolis-Hastings MCMC, которую я применял в своих расчетах.</p>
<p>В виде формул я уже описывал этот алгоритм в своей статье: <b><i>Султанов И.Р. Прогнозирование спредов доходности на первичном рынке российских рублевых корпоративных облигаций // Горизонты экономики. - 2019. - Vol. 1 (47). - P. 62–73.</i></b> (<a href="https://elibrary.ru/item.asp?id=37106647" target="_blank">ссылка на публикацию</a>)
<br>Здесь приводится непосредственно реализация алгоритма (исходный код в R).
<br>Возможно, кому то пригодится. Удачи в ваших исследованиях. </p>
<p>Для написания собственной реализации алгоритма за основу взял:
<ol>
<li>
пример реализации алгоритма MCMC для случая одной переменной: <b><i>Wiecki T. MCMC sampling for dummies. Available at: <a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/" target="_blank">http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/ </a>(accessed 20.06.2017).</i></b>
</li><li>
формулы для расчетов из статьи: <b><i>Айвазян С.А. Байесовский подход в эконометрическом анализе // Прикладная эконометрика. - 2008. - Vol. 9. - Is. 1. - P. 93–130.</i></b></li>
</ol>
</p>
<p>
В общем виде оцениваемая эконометрическая модель может быть записана как:
{% raw %}\[y={{\theta }_{0}}+{{\theta }_{1}}*{{x}^{(1)}}+{{\theta }_{2}}*{{x}^{(2)}}+...+{{\theta }_{k}}*{{x}^{(k)}}+\varepsilon \]{% endraw %}
</p><p>
В матричной записи формула принимает вид:
{% raw %}$$Y=X\theta +\varepsilon $${% endraw %}
где {% raw %}\(Y={{\left( {{y}_{1}},{{y}_{2}},\ldots ,{{y}_{n}} \right)}^{T}}\){% endraw %}
– значения зависимой переменной,
{% raw %}\(n\){% endraw %}
– число наблюдений;
{% raw %}\(X=\left[ \begin{matrix}
   1 & x_{1}^{\left( 1 \right)} & \ldots  & x_{1}^{\left( k \right)}  \\
   1 & x_{2}^{\left( 1 \right)} & \ldots  & x_{2}^{\left( k \right)}  \\
   \ldots  & \ldots  & \ldots  & \ldots   \\
   1 & x_{n}^{\left( 1 \right)} & \ldots  & x_{n}^{\left( k \right)}  \\
\end{matrix} \right]\){% endraw %}
– значения независимых переменных (верхний индекс – номер переменной, нижний индекс – номер наблюдения);
{% raw %}\(\theta ={{\left( {{\theta }_{0}},{{\theta }_{1}},\ldots ,{{\theta }_{k}} \right)}^{T}}\){% endraw %}
– значения параметров модели,
{% raw %}\(k\){% endraw %}
– число переменных;
{% raw %}\(\varepsilon ={{\left( {{\varepsilon }_{1}},{{\varepsilon }_{2}},\ldots ,{{\varepsilon }_{n}} \right)}^{T}}\){% endraw %}
– значения остатков модели (регрессии);
{% raw %}\(\varepsilon \in {{N}_{n}}\left( 0;\frac{1}{h}{{I}_{n}} \right)\){% endraw %}
– распределение остатков,
где {% raw %}\(h=\frac{1}{D\varepsilon }\){% endraw %},
{% raw %}\(D\varepsilon =var\left( \varepsilon  \right)\){% endraw %},
{% raw %}\({{I}_{n}}\){% endraw %} – единичная матрица размерности {% raw %}\(n\){% endraw %}.</p>

<hr>
<p>
Для определения первоначальных значений параметров, cначала рассчитывались оценки коэффициентов модели и
остатки регрессии обычным методом МНК (метод наименьших квадратов) на данных за
предыдущие периоды (годы).</p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_1')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_1" class="code_lines_hide">
<pre><code class="language-R line-numbers">#estimate linear regressions
m1 &lt;- lm(f, mydata, year==2007)
m2 &lt;- lm(f, mydata, year==2008)
m3 &lt;- lm(f, mydata, year==2009)
m4 &lt;- lm(f, mydata, year==2010)
m5 &lt;- lm(f, mydata, year==2011)
m6 &lt;- lm(f, mydata, year==2012)
m7 &lt;- lm(f, mydata, year==2013)
m8 &lt;- lm(f, mydata, year==2014)

#get thetas from coefficients
cf1 &lt;- coef(m1)
cf2 &lt;- coef(m2)
cf3 &lt;- coef(m3)
cf4 &lt;- coef(m4)
cf5 &lt;- coef(m5)
cf6 &lt;- coef(m6)
cf7 &lt;- coef(m7)
cf8 &lt;- coef(m8)</code></pre>
</div></div>
<!-- Продолжение -->
<hr>
<p>
Затем, начальные значения параметров брались, как
{% raw %}\({{\theta }_{i}}=\frac{\sum\limits_{j=1}^{s}{\theta _{i}^{t-j}}}{s}\){% endraw %}
– начальное значение коэффициента {% raw %}\(i\){% endraw %}
(верхний индекс обозначает номер года, {% raw %}\(\theta _{i}^{t-j}\){% endraw %}
- параметр {% raw %}\(i\){% endraw %} оцененный на данных за год {% raw %}\(t - j)\){% endraw %};
{% raw %}\(h=\frac{\sum\limits_{j=1}^{s}{{{h}^{t-j}}}}{s}\){% endraw %}
– (верхний индекс обозначает номер года);
{% raw %}\({{\sigma }_{i}}=\sqrt{D{{\theta }_{i}}}\){% endraw %};
{% raw %}\({{\sigma }_{h}}=\sqrt{Dh}\){% endraw %};
({% raw %}\(h=\frac{1}{D\varepsilon }\){% endraw %};
{% raw %}\(D\varepsilon =var\left( \varepsilon  \right)\){% endraw %}).
</p>
<div class="code">
  <div class="code_header">
    <a onclick="show_hide('code_2')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
  </div>
  <div id="code_2" class="code_lines_hide">
<pre><code class="language-R line-numbers">#primary estimations
hh &lt;- 1/c(var(residuals(m1)),var(residuals(m2)),
          var(residuals(m3)),var(residuals(m4)),
          var(residuals(m5)),var(residuals(m6)),
          var(residuals(m7)),var(residuals(m8)))
h &lt;- mean(hh)
st_h &lt;- sqrt(var(hh))</code></pre>
</div></div>
<!-- Продолжение -->
<hr>
<p>Параметры модели (т.е. переменные {% raw %}\(\theta \){% endraw %} и {% raw %}\(h\){% endraw %}) рассматривались, как показатели,
подчиняющиеся многомерному гамма-нормальному распределению
{% raw %}\(E\theta =Et(2a|\theta ;\frac{\alpha }{\beta }\Lambda )\){% endraw %},
где {% raw %}\(t(…,…)\){% endraw %} – многомерное обобщенное распределение Стьюдента.
Параметры распределения рассчитывались как
{% raw %}\(\alpha =\frac{{{h}^{2}}}{{{\sigma }_{h}}^{2}}\){% endraw %};
{% raw %}\(\beta =\frac{h}{{{\sigma }_{h}}^{2}}\){% endraw %};
{% raw %}\(\Lambda =\left[ \begin{matrix}
   {{\lambda }^{\left( 0 \right)}} & 0 & \ldots  & 0  \\
   0 & {{\lambda }^{\left( 1 \right)}} & \ldots  & 0  \\
   \ldots  & \ldots  & \ldots  & \ldots   \\
   0 & 0 & \ldots  & {{\lambda }^{\left( k \right)}}  \\
\end{matrix} \right]\){% endraw %} (25).
Где {% raw %}\({{\lambda }^{\left( j \right)}}=\left| \frac{1}{{{\sigma }_{j}}^{2}}\cdot \frac{\beta }{\alpha -1} \right|\){% endraw %}
</p>
<div class="code">
  <div class="code_header">
    <a onclick="show_hide('code_3')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
  </div>
<div id="code_3" class="code_lines_hide">
<pre><code class="language-R line-numbers">#calc the distribution params
alpha &lt;- (h^2)/(st_h^2)
beta &lt;- h/(st_h^2)

ll &lt;- length(cf1)

cfl &lt;- matrix(0, ll, 8)
th &lt;- matrix(0, ll, 1)
st_th &lt;- matrix(0, ll, 1)
#L matrix calculation
L &lt;- matrix(0, ll, ll)
for(i in 1:ll)
{
  cfl[i,] &lt;- c(cf1[i],cf2[i],cf3[i],cf4[i],cf5[i],
               cf6[i],cf7[i],cf8[i])
  th[i] &lt;- mean(cfl[i,])
  st_th[i] &lt;- sqrt(var(cfl[i,]))
  L[i,i] &lt;- (1/(st_th[i]^2))*(beta/(alpha-1))
}</code></pre>
</div></div>
<!-- Продолжение -->
<hr>
<p>
Далее параметры модели оценивались на новых данных.
Для расчетов методом Metropolis-Hastings MCMC применялась функция с названием <b>mcmc</b>
речь о которой пойдет далее.
</p>
<div class="code">
  <div class="code_header">
    <a onclick="show_hide('code_4')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
  </div>
<div id="code_4" class="code_lines_hide">
<pre><code class="language-R line-numbers">#get X and Y
ndata &lt;- subset(mydata, year==2015)
M &lt;- as.matrix(model.frame(formula=f, data=ndata))
Y &lt;- M[,1]
X &lt;- cbind(1,M[,-1])
n &lt;- length(Y)

iter &lt;- 10000
data &lt;- mcmc(iter, X, Y, th, alpha, beta, h, L)

#get model parameters from MCMC estimation
th_sim &lt;- matrix(0,ncol(data)-1,1)
for(i in 2:ncol(data)) th_sim[i-1] &lt;- rbind(mean(data[,i]))</code></pre>
</div></div>
<!-- Продолжение -->
<hr>
<p>
<b>Содержимое функции mcmc.</b>
Первоначальные значения параметров перед запуском алгоритма Metropolis-Hastings MCMC рассчитывались, как:
{% raw %}\({{S}_{a}}=w*S\){% endraw %};
{% raw %}\(S=\left( {{S}_{1}},\ldots ,{{S}_{i}} \right)\){% endraw %};
{% raw %}\({{S}_{i}}=\frac{1}{\sqrt{{{c}_{i}}}}\){% endraw %};
{% raw %}\({{c}_{i}}={{b}_{ii}}-{{B}_{i.}}B{{\left( i \right)}^{-1}}{{B}_{.i}}\){% endraw %};
{% raw %}\(B=\left[ \begin{matrix}
     {{b}_{ii}} & {{B}_{i.}}  \\
     {{B}_{.i}} & B\left( i \right)  \\
  \end{matrix} \right]\){% endraw %};
  {% raw %}\(B=\frac{\alpha }{\beta }\Lambda \);
  \(w=1\){% endraw %} (позволяет регулировать процент принятия новых значений или acceptance rate).
</p><p>
<div class="code">
<div class="code_header">
    <a onclick="show_hide('code_5')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
  </div>
<div id="code_5" class="code_lines_hide">
<pre><code class="language-R line-numbers">#preliminary estimation 1
B &lt;- h*L
C &lt;- matrix(0, length(b_TH), 1)
for(i in 1:length(b_TH))
{
  C[i] &lt;- B[i,i]-B[i,-i]%*%solve(B[-i,-i],B[-i,i])
}
stdp &lt;- 1/sqrt(C)</code></pre>
</div></div>
</p>
<!-- Продолжение -->
<hr>
<p>
Первоначальные значения элементов цепи (до запуска алгоритма) рассчитывались следующим образом:
{% raw %}\(~M=\left( {{M}_{1}},\ldots ,{{M}_{q}} \right)\){% endraw %};
{% raw %}\({{Y}_{p}}=X\theta \){% endraw %};
{% raw %}\({{\vartheta }_{pr}}=var\left( Y-{{Y}_{p}} \right)\){% endraw %};
{% raw %}\({{M}_{1}}=\left( {{\vartheta }_{pr}};\theta  \right)\){% endraw %};
{% raw %}\({{P}_{pr}}=apost\left( {{Y}_{p}};{{\vartheta }_{pr}};\theta  \right)\){% endraw %};
{% raw %}\({{\theta }_{pv}}=\theta \){% endraw %}.
Где {% raw %}\(M\){% endraw %} – марковская цепь; {% raw %}\(q\){% endraw %} – длина цепи;
{% raw %}\(apost(…;…;…)\){% endraw %}
– функция расчета апостериорной вероятности (в программе названа как aposterior).
</p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_7')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_7" class="code_lines_hide">
<pre><code  class="language-R line-numbers">#preliminary estimation 2
iter &lt;- iter*1.1
b &lt;- b_TH
pY &lt;- X%*%b
resid &lt;- Y-pY
sig_sq &lt;- as.numeric(var(resid))
output=matrix(0,ncol=ncol(X)+1,nrow=iter)
output[1,]=c(sig_sq,b)

prevp &lt;- aposterior(b, sig_sq, b_TH, alpha, beta, stdp, Y, pY)</code></pre>
</div></div>
<!-- Продолжение --->
<hr>
<p>
Далее, начиная со второго элемента, каждый следующий элемент последовательно рассчитывался так:
{% raw %}\({{\theta }^{*}}=prop\left( {{\theta }_{pv}};{{S}_{a}} \right)\){% endraw %};
{% raw %}\({{Y}_{p}}=X{{\theta }^{*}}\){% endraw %};
{% raw %}\({{\vartheta }^{*}}=var\left( Y-{{Y}_{p}} \right)\){% endraw %}; если
{% raw %}\(r&lt;\frac{apost\left( {{Y}_{p}}\;{{\vartheta }^{*}};{{\theta }^{*}} \right)}{{{P}_{pr}}}\){% endraw %},
то {% raw %}{\({{M}_{i}}=\left( {{\vartheta }^{*}};{{\theta }^{*}} \right)\); \({{\vartheta }_{pr}}={{\vartheta }^{*}}\){% endraw %};
{% raw %}\({{\theta }_{pv}}={{\theta }^{*}}\); \({{P}_{pr}}=apost\left( {{Y}_{p}};{{\vartheta }^{*}};{{\theta }^{*}} \right)\)}{% endraw %}
иначе {% raw %}{\({{M}_{i}}=\left( {{\vartheta }_{pr}};{{\theta }_{pv}} \right)\){% endraw %}},
где {% raw %}\(r\){% endraw %} – случайное число из равномерного распределения на отрезке {% raw %}\([0;1]\){% endraw %}};
{% raw %}\(prop\left( {{\theta }_{pv}};{{S}_{a}} \right)\){% endraw %} – функция,
случайным образом генерирующая вектор чисел из нормального распределения со средними
{% raw %}\({{\theta }_{pv}}\){% endraw %} и стандартными отклонениями   (при этом предполагается, что отдельные числа входящие в вектор независимы между собой).
При практических расчетах значения вероятностей заменялись значениями логарифмов вероятности,
так как в этом случае деление вероятностей заменяется вычитанием логарифмов вероятностей и расчеты происходят значительно быстрее.
</p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_8')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_8" class="code_lines_hide">
<pre><code  class="language-R line-numbers">#mcmc algorithm main cycle
i &lt;- 2
while(i&lt;=iter)
{
  bn &lt;- propose(b, stdp)
  pY &lt;- X%*%bn
  residn &lt;- Y-pY
  sig_sqn &lt;- as.numeric(var(residn))
  ace &lt;- aposterior(bn, sig_sqn, b_TH, alpha, beta, stdp, Y, pY)
  if(log(runif(1))&lt;(ace-prevp)) {
    b &lt;- bn
    sig_sq &lt;- sig_sqn
    prevp &lt;- ace
  }
  output[i,]=c(sig_sq,b)
  i &lt;- i+1
}
</code></pre>
</div></div>
<hr>
<p><b>Функция mcmc, полностью:</b></p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_9')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_9" class="code_lines_hide">
<pre><code  class="language-R line-numbers">#mcmc simulation
#iter - number of iterations
#X, Y - curent data
#b_TH - vector of coeffients from previous data estimation
#alpha, beta - gamma distribution parameters from previous data estimation
#h, L - accuracy variables from previous data estimation
mcmc &lt;- function(iter, X, Y, b_TH, alpha, beta, h, L)
{
  #preliminary estimation 1
  B &lt;- h*L
  C &lt;- matrix(0, length(b_TH), 1)
  for(i in 1:length(b_TH))
  {
    C[i] &lt;- B[i,i]-B[i,-i]%*%solve(B[-i,-i],B[-i,i])
  }
  stdp &lt;- 1/sqrt(C)

  #preliminary estimation 2
  iter &lt;- iter*1.1
  b &lt;- b_TH
  pY &lt;- X%*%b
  resid &lt;- Y-pY
  sig_sq &lt;- as.numeric(var(resid))
  output=matrix(0,ncol=ncol(X)+1,nrow=iter)
  output[1,]=c(sig_sq,b)

  prevp &lt;- aposterior(b, sig_sq, b_TH, alpha, beta, stdp, Y, pY)

  #mcmc algorithm main cycle
  i &lt;- 2
  while(i&lt;=iter)
  {
    bn &lt;- propose(b, stdp)
    pY &lt;- X%*%bn
    residn &lt;- Y-pY
    sig_sqn &lt;- as.numeric(var(residn))
    ace &lt;- aposterior(bn, sig_sqn, b_TH, alpha, beta, stdp, Y, pY)
    if(log(runif(1))&lt;(ace-prevp)) {
      b &lt;- bn
      sig_sq &lt;- sig_sqn
      prevp &lt;- ace
    }
    output[i,]=c(sig_sq,b)
    i &lt;- i+1
  }
  return(output[-c(1:(iter/11)),])
}
</code></pre>
</div></div>
<hr>
<p>Функция {% raw %}\(prop(...,...)\){% endraw %} (в программе названа как propose) функция,
  случайным образом генерирующая вектор чисел из нормального распределения.</p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_10')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_10" class="code_lines_hide">
<pre><code  class="language-R line-numbers">#generate new varibles
propose &lt;- function(b, st)
{
  return(rnorm(length(b),mean=b,sd=st))
}
</code></pre>
</div></div>
<hr>
<p>
Функция {% raw %}\(apost(…;…;…)\){% endraw %}, упоминавшаяся ранее, считается следующим образом:
{% raw %}\(apost\left( {{Y}_{p}};{{\vartheta }^{*}};{{\theta }^{*}} \right)=prior\left( {{\vartheta }^{*}};{{\theta }^{*}} \right)*Like\left( {{Y}_{p}} \right)\){% endraw %};
где {% raw %}\(prior\left( {{\vartheta }^{*}};{{\theta }^{*}} \right)={{P}_{g}}\left( \frac{1}{{{\vartheta }^{*}}};\alpha ;\frac{1}{\beta } \right)*\underset{i=0}{\overset{k}{\mathop \prod }}\,{{P}_{n}}\left( {{\theta }^{*}}_{i};{{\theta }_{i}};{{S}_{i}} \right)\){% endraw %}
– априорная вероятность;
{% raw %}\(Like\left( {{Y}_{p}} \right)=\underset{i=1}{\overset{n}{\mathop \prod }}\,{{P}_{n}}\left( {{Y}_{p}}_{i};{{Y}_{i}};\sqrt{var\left( Y \right)} \right)\){% endraw %}
– функция правдоподобия;
{% raw %}\({{P}_{g}}\left( \frac{1}{{{\vartheta }^{*}}};\alpha ;\frac{1}{\beta } \right)\){% endraw %}
– функция, возвращающая значения функции гамма-распределения с параметрами
{% raw %}\(\alpha\) и \(\frac{1}{\beta }\); \({{P}_{n}}\left( {{\theta }^{*}}_{i};{{\theta }_{i}};{{S}_{i}} \right)\){% endraw %}
– функция, возвращающая значения функции нормального распределения с средней
{% raw %}\({{\theta }_{i}}\){% endraw %} и стандартным отклонением
{% raw %}\({{S}_{i}}\){% endraw %}.
При практических расчетах значения вероятностей заменялись значениями логарифмов вероятности,
так как в этом случае перемножение вероятностей заменяется суммированием логарифмов вероятностей
и расчеты происходят значительно быстрее.
</p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_11')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_11" class="code_lines_hide">
<pre><code  class="language-R line-numbers">#aprior likelyhood
prior &lt;- function(b, sig_sq, b_TH, alpha, beta, stdp)
{
  dn &lt;- dnorm(b, mean=b_TH, sd=stdp, log=T)
  dg &lt;- dgamma(1/sig_sq, shape=alpha, scale=1/beta, log=T)
  return(sum(dn)+dg)
}

#likelyhood function
likelyhood &lt;- function(b, Y, pY)
{
  st &lt;- sqrt(var(pY))
  dn &lt;- dnorm(Y, mean = pY, sd = st, log = T)
  return(sum(dn))
}

#aposterior likelyhood
aposterior &lt;- function(b, sig_sq, b_TH, alpha, beta, stdp, Y, pY)
{
  return(prior(b, sig_sq, b_TH, alpha, beta, stdp)+likelyhood(b, Y, pY))
}
</code></pre>
</div></div>
<hr>
<p><b><i>Код программы полностью (<a href="https://github.com/BigIskander/mcmc" target="_blank">скачать</a>).</i></b></p>
<div class="code">
<div class="code_header">
  <a onclick="show_hide('code_12')" href="javascript:void(0);">Код в R (<em>показать</em>):</a>
</div>
<div id="code_12" class="code_lines_hide">
<pre data-src="https://github.com/BigIskander/mcmc" data-download-link><code  class="language-R line-numbers">#Metropolis-Hastings MCMC algorithm implementation
#https://bigiskander.github.io/mcmc.html
#author: Sultanov Iskander
# &#66;&#105;&#103;&#73;&#115;&#107;&#97;&#110;&#100;&#101;&#114;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;

#aprior likelyhood
prior &lt;- function(b, sig_sq, b_TH, alpha, beta, stdp)
{
  dn &lt;- dnorm(b, mean=b_TH, sd=stdp, log=T)
  dg &lt;- dgamma(1/sig_sq, shape=alpha, scale=1/beta, log=T)
  return(sum(dn)+dg)
}

#likelyhood function
likelyhood &lt;- function(b, Y, pY)
{
  st &lt;- sqrt(var(pY))
  dn &lt;- dnorm(Y, mean = pY, sd = st, log = T)
  return(sum(dn))
}

#aposterior likelyhood
aposterior &lt;- function(b, sig_sq, b_TH, alpha, beta, stdp, Y, pY)
{
  return(prior(b, sig_sq, b_TH, alpha, beta, stdp)+likelyhood(b, Y, pY))
}

#generate new varibles
propose &lt;- function(b, st)
{
  return(rnorm(length(b),mean=b,sd=st))
}

#mcmc simulation
#iter - number of iterations
#X, Y - curent data
#b_TH - vector of coeffients from previous data estimation
#alpha, beta - gamma distribution parameters from previous data estimation
#h, L - accuracy variables from previous data estimation
mcmc &lt;- function(iter, X, Y, b_TH, alpha, beta, h, L)
{
  #preliminary estimation 1
  B &lt;- h*L
  C &lt;- matrix(0, length(b_TH), 1)
  for(i in 1:length(b_TH))
  {
    C[i] &lt;- B[i,i]-B[i,-i]%*%solve(B[-i,-i],B[-i,i])
  }
  stdp &lt;- 1/sqrt(C)

  #preliminary estimation 2
  iter &lt;- iter*1.1
  b &lt;- b_TH
  pY &lt;- X%*%b
  resid &lt;- Y-pY
  sig_sq &lt;- as.numeric(var(resid))
  output=matrix(0,ncol=ncol(X)+1,nrow=iter)
  output[1,]=c(sig_sq,b)

  prevp &lt;- aposterior(b, sig_sq, b_TH, alpha, beta, stdp, Y, pY)

  #mcmc algorithm main cycle
  i &lt;- 2
  while(i&lt;=iter)
  {
    bn &lt;- propose(b, stdp)
    pY &lt;- X%*%bn
    residn &lt;- Y-pY
    sig_sqn &lt;- as.numeric(var(residn))
    ace &lt;- aposterior(bn, sig_sqn, b_TH, alpha, beta, stdp, Y, pY)
    if(log(runif(1))&lt;(ace-prevp)) {
      b &lt;- bn
      sig_sq &lt;- sig_sqn
      prevp &lt;- ace
    }
    output[i,]=c(sig_sq,b)
    i &lt;- i+1
  }
  return(output[-c(1:(iter/11)),])
}

#dencity plot
dence &lt;- function(data)
{
  cc &lt;- ncol(data)
  if(cc&gt;3)
  {
    mm &lt;- round(cc/3,0)
    if(mm&lt;cc/3) mm &lt;- mm+1
    par(mfrow=c(mm,3))
  } else {
    par(mfrow=c(1,cc))
  }
  #dencity
  for(i in 2:cc)
  {
    hist(data[,i],breaks=150,main=paste0("th",(i-2)),freq=FALSE)
    lines(density(data[,i],bw="nrd",
                  kernel="gaussian"),col="red",lwd=2)
  }
  i &lt;- 1
  hist(data[,i],breaks=150,main="sig_sq",freq=FALSE)
  lines(density(data[,1],bw="nrd",kernel="gaussian"),col="red",lwd=2)
}

#example how you can calculate variables necessary for the algorithm
#and make calculations using this implementation of MCMC algorithm


#load the data
library(foreign)
#file with data saved in stata format (not provided)
mydata &lt;- read.dta("D:/Учеба/ВШЭ Аспирантура/Science/Calc in Stata/470.dta")
#econometric model for estimation (for example only)
f &lt;- "YS~Leverage+LN_Maturity+LN_Issue"

# 2014 excluded because of too smal number of observations
#estimate linear regressions
m1 &lt;- lm(f, mydata, year==2007)
m2 &lt;- lm(f, mydata, year==2008)
m3 &lt;- lm(f, mydata, year==2009)
m4 &lt;- lm(f, mydata, year==2010)
m5 &lt;- lm(f, mydata, year==2011)
m6 &lt;- lm(f, mydata, year==2012)
m7 &lt;- lm(f, mydata, year==2013)
#m8 &lt;- lm(f, mydata, year==2014)

#get coefficients (theta)
cf1 &lt;- coef(m1)
cf2 &lt;- coef(m2)
cf3 &lt;- coef(m3)
cf4 &lt;- coef(m4)
cf5 &lt;- coef(m5)
cf6 &lt;- coef(m6)
cf7 &lt;- coef(m7)
#cf8 &lt;- coef(m8)

#primary estimations
hh &lt;- 1/c(var(residuals(m1)),var(residuals(m2)),
          var(residuals(m3)),var(residuals(m4)),
          var(residuals(m5)),var(residuals(m6)),
          var(residuals(m7)))#,var(residuals(m8)))
h &lt;- mean(hh)
st_h &lt;- sqrt(var(hh))

#calc the distribution params
alpha &lt;- (h^2)/(st_h^2)
beta &lt;- h/(st_h^2)

ll &lt;- length(cf1)

cfl &lt;- matrix(0, ll, 7) #8
th &lt;- matrix(0, ll, 1)
st_th &lt;- matrix(0, ll, 1)
#L matrix calculation
L &lt;- matrix(0, ll, ll)
for(i in 1:ll)
{
  cfl[i,] &lt;- c(cf1[i],cf2[i],cf3[i],cf4[i],cf5[i],
               cf6[i],cf7[i])#,cf8[i])
  th[i] &lt;- mean(cfl[i,])
  st_th[i] &lt;- sqrt(var(cfl[i,]))
  L[i,i] &lt;- (1/(st_th[i]^2))*(beta/(alpha-1))
}

#get X and Y
ndata &lt;- subset(mydata, year==2015)
M &lt;- as.matrix(model.frame(formula=f, data=ndata))
Y &lt;- M[,1]
X &lt;- cbind(1,M[,-1])
n &lt;- length(Y)

#estimate using MCMC
iter &lt;- 1000000
data &lt;- mcmc(iter, X, Y, th, alpha, beta, h, L)

#get model parameters from MCMC estimation
th_sim &lt;- matrix(0,ncol(data)-1,1)
for(i in 2:ncol(data)) th_sim[i-1] &lt;- rbind(mean(data[,i]))
print(th_sim)

#plot dencity graphs
dence(data)
</code></pre>
</div></div>
<hr>
<p>Результат вычислений на моих данных приведенным выше кодом (только для примера):</p>
<p><img src="/assets/images/mcmc/values.PNG" class="no_zoom"></p>
<p><img src="/assets/images/mcmc/Rplot.png" class="no_zoom"></p>
